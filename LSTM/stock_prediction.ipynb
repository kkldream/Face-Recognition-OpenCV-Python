{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kk/jupyter/stock_prediction\n"
     ]
    }
   ],
   "source": [
    "cd /home/kk/jupyter/stock_prediction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrain():\n",
    "  train = pd.read_csv(\"SPY.csv\")\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augFeatures(train):\n",
    "  train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "  train[\"year\"] = train[\"Date\"].dt.year\n",
    "  train[\"month\"] = train[\"Date\"].dt.month\n",
    "  train[\"date\"] = train[\"Date\"].dt.day\n",
    "  train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "  train = train.drop([\"Date\"], axis=1)\n",
    "  train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "  return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "  X_train, Y_train = [], []\n",
    "  for i in range(train.shape[0]-futureDay-pastDay):\n",
    "    X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "    Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "  return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "  np.random.seed(10)\n",
    "  randomList = np.arange(X.shape[0])\n",
    "  np.random.shuffle(randomList)\n",
    "  return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "  X_train = X[int(X.shape[0]*rate):]\n",
    "  Y_train = Y[int(Y.shape[0]*rate):]\n",
    "  X_val = X[:int(X.shape[0]*rate)]\n",
    "  Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "  return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Augment the features (year, month, date, day)\n",
    "train_Aug = augFeatures(train)\n",
    "\n",
    "# Normalization\n",
    "train_norm = normalize(train_Aug)\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "# X_trian: (5710, 30, 10)\n",
    "# Y_train: (5710, 5, 1)\n",
    "# X_val: (634, 30, 10)\n",
    "# Y_val: (634, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (5684, 30, 10)\n",
      "Y_train: (5684, 1)\n",
      "X_val: (631, 30, 10)\n",
      "Y_val: (631, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'Y_train: {Y_train.shape}')\n",
    "print(f'X_val: {X_val.shape}')\n",
    "print(f'Y_val: {Y_val.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kk/jupyter/jupyter_env/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/kk/jupyter/jupyter_env/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(30, 10))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5684/5684 [==============================] - 2s 400us/step - loss: 0.1258 - val_loss: 0.0487\n",
      "Epoch 2/1000\n",
      "5684/5684 [==============================] - 2s 305us/step - loss: 0.0197 - val_loss: 0.0065\n",
      "Epoch 3/1000\n",
      "5684/5684 [==============================] - 2s 289us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 4/1000\n",
      "5684/5684 [==============================] - 2s 287us/step - loss: 0.0011 - val_loss: 8.5607e-04\n",
      "Epoch 5/1000\n",
      "5684/5684 [==============================] - 2s 291us/step - loss: 6.9623e-04 - val_loss: 5.9021e-04\n",
      "Epoch 6/1000\n",
      "5684/5684 [==============================] - 2s 306us/step - loss: 5.0994e-04 - val_loss: 4.5799e-04\n",
      "Epoch 7/1000\n",
      "5684/5684 [==============================] - 2s 327us/step - loss: 4.0347e-04 - val_loss: 3.7325e-04\n",
      "Epoch 8/1000\n",
      "5684/5684 [==============================] - 2s 332us/step - loss: 3.3263e-04 - val_loss: 3.1364e-04\n",
      "Epoch 9/1000\n",
      "5684/5684 [==============================] - 2s 297us/step - loss: 2.8097e-04 - val_loss: 2.7138e-04\n",
      "Epoch 10/1000\n",
      "5684/5684 [==============================] - 2s 281us/step - loss: 2.4381e-04 - val_loss: 2.3435e-04\n",
      "Epoch 11/1000\n",
      "5684/5684 [==============================] - 2s 310us/step - loss: 2.1406e-04 - val_loss: 2.0877e-04\n",
      "Epoch 12/1000\n",
      "5684/5684 [==============================] - 2s 357us/step - loss: 1.9140e-04 - val_loss: 1.8731e-04\n",
      "Epoch 13/1000\n",
      "5684/5684 [==============================] - 2s 348us/step - loss: 1.7171e-04 - val_loss: 1.7334e-04\n",
      "Epoch 14/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 1.5642e-04 - val_loss: 1.5728e-04\n",
      "Epoch 15/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 1.4214e-04 - val_loss: 1.4281e-04\n",
      "Epoch 16/1000\n",
      "5684/5684 [==============================] - 2s 350us/step - loss: 1.3012e-04 - val_loss: 1.3196e-04\n",
      "Epoch 17/1000\n",
      "5684/5684 [==============================] - 2s 352us/step - loss: 1.2052e-04 - val_loss: 1.2250e-04\n",
      "Epoch 18/1000\n",
      "5684/5684 [==============================] - 2s 338us/step - loss: 1.1188e-04 - val_loss: 1.1434e-04\n",
      "Epoch 19/1000\n",
      "5684/5684 [==============================] - 2s 356us/step - loss: 1.0494e-04 - val_loss: 1.0694e-04\n",
      "Epoch 20/1000\n",
      "5684/5684 [==============================] - 2s 360us/step - loss: 9.8725e-05 - val_loss: 1.0190e-04\n",
      "Epoch 21/1000\n",
      "5684/5684 [==============================] - 2s 355us/step - loss: 9.2427e-05 - val_loss: 9.6383e-05\n",
      "Epoch 22/1000\n",
      "5684/5684 [==============================] - 2s 347us/step - loss: 8.6889e-05 - val_loss: 9.0869e-05\n",
      "Epoch 23/1000\n",
      "5684/5684 [==============================] - 2s 345us/step - loss: 8.2442e-05 - val_loss: 8.6112e-05\n",
      "Epoch 24/1000\n",
      "5684/5684 [==============================] - 2s 359us/step - loss: 7.8904e-05 - val_loss: 8.6121e-05\n",
      "Epoch 25/1000\n",
      "5684/5684 [==============================] - 2s 356us/step - loss: 7.5351e-05 - val_loss: 7.8960e-05\n",
      "Epoch 26/1000\n",
      "5684/5684 [==============================] - 2s 350us/step - loss: 7.1499e-05 - val_loss: 7.5797e-05\n",
      "Epoch 27/1000\n",
      "5684/5684 [==============================] - 2s 357us/step - loss: 6.9154e-05 - val_loss: 7.3222e-05\n",
      "Epoch 28/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 6.6445e-05 - val_loss: 7.1216e-05\n",
      "Epoch 29/1000\n",
      "5684/5684 [==============================] - 2s 355us/step - loss: 6.5554e-05 - val_loss: 7.0136e-05\n",
      "Epoch 30/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 6.3164e-05 - val_loss: 6.6878e-05\n",
      "Epoch 31/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 6.2061e-05 - val_loss: 6.5775e-05\n",
      "Epoch 32/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 5.9803e-05 - val_loss: 6.4483e-05\n",
      "Epoch 33/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 5.8370e-05 - val_loss: 6.2245e-05\n",
      "Epoch 34/1000\n",
      "5684/5684 [==============================] - 2s 357us/step - loss: 5.7147e-05 - val_loss: 6.1877e-05\n",
      "Epoch 35/1000\n",
      "5684/5684 [==============================] - 2s 356us/step - loss: 5.6856e-05 - val_loss: 6.1331e-05\n",
      "Epoch 36/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 5.5089e-05 - val_loss: 6.0646e-05\n",
      "Epoch 37/1000\n",
      "5684/5684 [==============================] - 2s 333us/step - loss: 5.4633e-05 - val_loss: 5.9147e-05\n",
      "Epoch 38/1000\n",
      "5684/5684 [==============================] - 2s 343us/step - loss: 5.3406e-05 - val_loss: 5.7580e-05\n",
      "Epoch 39/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 5.2819e-05 - val_loss: 5.8852e-05\n",
      "Epoch 40/1000\n",
      "5684/5684 [==============================] - 2s 348us/step - loss: 5.2944e-05 - val_loss: 5.6317e-05\n",
      "Epoch 41/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 5.1209e-05 - val_loss: 5.6671e-05\n",
      "Epoch 42/1000\n",
      "5684/5684 [==============================] - 2s 356us/step - loss: 5.1334e-05 - val_loss: 5.5222e-05\n",
      "Epoch 43/1000\n",
      "5684/5684 [==============================] - 2s 347us/step - loss: 5.0890e-05 - val_loss: 5.4774e-05\n",
      "Epoch 44/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 5.0378e-05 - val_loss: 5.3878e-05\n",
      "Epoch 45/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 4.9772e-05 - val_loss: 5.4683e-05\n",
      "Epoch 46/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 4.9527e-05 - val_loss: 5.3634e-05\n",
      "Epoch 47/1000\n",
      "5684/5684 [==============================] - 2s 352us/step - loss: 4.8858e-05 - val_loss: 5.2623e-05\n",
      "Epoch 48/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 4.7412e-05 - val_loss: 5.1785e-05\n",
      "Epoch 49/1000\n",
      "5684/5684 [==============================] - 2s 352us/step - loss: 4.6995e-05 - val_loss: 5.1064e-05\n",
      "Epoch 50/1000\n",
      "5684/5684 [==============================] - 2s 355us/step - loss: 4.6967e-05 - val_loss: 5.0906e-05\n",
      "Epoch 51/1000\n",
      "5684/5684 [==============================] - 2s 349us/step - loss: 4.6311e-05 - val_loss: 5.1865e-05\n",
      "Epoch 52/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 4.5556e-05 - val_loss: 4.9967e-05\n",
      "Epoch 53/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 4.5504e-05 - val_loss: 4.9933e-05\n",
      "Epoch 54/1000\n",
      "5684/5684 [==============================] - 2s 358us/step - loss: 4.5110e-05 - val_loss: 4.9751e-05\n",
      "Epoch 55/1000\n",
      "5684/5684 [==============================] - 2s 355us/step - loss: 4.5107e-05 - val_loss: 5.0637e-05\n",
      "Epoch 56/1000\n",
      "5684/5684 [==============================] - 2s 344us/step - loss: 4.4336e-05 - val_loss: 4.9585e-05\n",
      "Epoch 57/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 4.4430e-05 - val_loss: 4.9750e-05\n",
      "Epoch 58/1000\n",
      "5684/5684 [==============================] - 2s 357us/step - loss: 4.4550e-05 - val_loss: 4.7968e-05\n",
      "Epoch 59/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 4.3638e-05 - val_loss: 4.6608e-05\n",
      "Epoch 60/1000\n",
      "5684/5684 [==============================] - 2s 347us/step - loss: 4.2913e-05 - val_loss: 4.6838e-05\n",
      "Epoch 61/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 4.2467e-05 - val_loss: 4.6762e-05\n",
      "Epoch 62/1000\n",
      "5684/5684 [==============================] - 2s 343us/step - loss: 4.2276e-05 - val_loss: 4.8533e-05\n",
      "Epoch 63/1000\n",
      "5684/5684 [==============================] - 2s 357us/step - loss: 4.2094e-05 - val_loss: 4.8825e-05\n",
      "Epoch 64/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 4.2584e-05 - val_loss: 4.7077e-05\n",
      "Epoch 65/1000\n",
      "5684/5684 [==============================] - 2s 338us/step - loss: 4.1305e-05 - val_loss: 4.5333e-05\n",
      "Epoch 66/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 4.1745e-05 - val_loss: 4.7980e-05\n",
      "Epoch 67/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 4.0855e-05 - val_loss: 4.4702e-05\n",
      "Epoch 68/1000\n",
      "5684/5684 [==============================] - 2s 341us/step - loss: 4.1655e-05 - val_loss: 4.5282e-05\n",
      "Epoch 69/1000\n",
      "5684/5684 [==============================] - 2s 327us/step - loss: 4.0128e-05 - val_loss: 4.4578e-05\n",
      "Epoch 70/1000\n",
      "5684/5684 [==============================] - 2s 321us/step - loss: 4.0080e-05 - val_loss: 4.3732e-05\n",
      "Epoch 71/1000\n",
      "5684/5684 [==============================] - 2s 349us/step - loss: 3.9076e-05 - val_loss: 4.3450e-05\n",
      "Epoch 72/1000\n",
      "5684/5684 [==============================] - 2s 329us/step - loss: 3.9243e-05 - val_loss: 4.4945e-05\n",
      "Epoch 73/1000\n",
      "5684/5684 [==============================] - 2s 350us/step - loss: 3.9652e-05 - val_loss: 4.2928e-05\n",
      "Epoch 74/1000\n",
      "5684/5684 [==============================] - 2s 349us/step - loss: 3.9584e-05 - val_loss: 4.4482e-05\n",
      "Epoch 75/1000\n",
      "5684/5684 [==============================] - 2s 317us/step - loss: 3.8819e-05 - val_loss: 4.4961e-05\n",
      "Epoch 76/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 3.8777e-05 - val_loss: 4.2721e-05\n",
      "Epoch 77/1000\n",
      "5684/5684 [==============================] - 2s 350us/step - loss: 3.9080e-05 - val_loss: 4.5024e-05\n",
      "Epoch 78/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 3.7492e-05 - val_loss: 4.1619e-05\n",
      "Epoch 79/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 3.7929e-05 - val_loss: 4.1879e-05\n",
      "Epoch 80/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 3.7924e-05 - val_loss: 4.1567e-05\n",
      "Epoch 81/1000\n",
      "5684/5684 [==============================] - 2s 350us/step - loss: 3.7862e-05 - val_loss: 4.1578e-05\n",
      "Epoch 82/1000\n",
      "5684/5684 [==============================] - 2s 350us/step - loss: 3.7181e-05 - val_loss: 4.2200e-05\n",
      "Epoch 83/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 3.7239e-05 - val_loss: 4.0964e-05\n",
      "Epoch 84/1000\n",
      "5684/5684 [==============================] - 2s 357us/step - loss: 3.6887e-05 - val_loss: 4.7856e-05\n",
      "Epoch 85/1000\n",
      "5684/5684 [==============================] - 2s 347us/step - loss: 3.7388e-05 - val_loss: 4.5904e-05\n",
      "Epoch 86/1000\n",
      "5684/5684 [==============================] - 2s 327us/step - loss: 3.6663e-05 - val_loss: 3.9884e-05\n",
      "Epoch 87/1000\n",
      "5684/5684 [==============================] - 2s 341us/step - loss: 3.7204e-05 - val_loss: 4.0405e-05\n",
      "Epoch 88/1000\n",
      "5684/5684 [==============================] - 2s 352us/step - loss: 3.5649e-05 - val_loss: 4.0520e-05\n",
      "Epoch 89/1000\n",
      "5684/5684 [==============================] - 2s 325us/step - loss: 3.5422e-05 - val_loss: 3.8930e-05\n",
      "Epoch 90/1000\n",
      "5684/5684 [==============================] - 2s 341us/step - loss: 3.5358e-05 - val_loss: 3.9838e-05\n",
      "Epoch 91/1000\n",
      "5684/5684 [==============================] - 2s 353us/step - loss: 3.6663e-05 - val_loss: 3.8865e-05\n",
      "Epoch 92/1000\n",
      "5684/5684 [==============================] - 2s 355us/step - loss: 3.5888e-05 - val_loss: 4.0755e-05\n",
      "Epoch 93/1000\n",
      "5684/5684 [==============================] - 2s 351us/step - loss: 3.5316e-05 - val_loss: 3.9851e-05\n",
      "Epoch 94/1000\n",
      "5684/5684 [==============================] - 2s 337us/step - loss: 3.4864e-05 - val_loss: 3.7789e-05\n",
      "Epoch 95/1000\n",
      "5684/5684 [==============================] - 2s 348us/step - loss: 3.4575e-05 - val_loss: 3.7839e-05\n",
      "Epoch 96/1000\n",
      "5684/5684 [==============================] - 2s 344us/step - loss: 3.4432e-05 - val_loss: 3.8022e-05\n",
      "Epoch 97/1000\n",
      "5684/5684 [==============================] - 2s 321us/step - loss: 3.4782e-05 - val_loss: 3.9814e-05\n",
      "Epoch 98/1000\n",
      "5684/5684 [==============================] - 2s 289us/step - loss: 3.4250e-05 - val_loss: 3.9711e-05\n",
      "Epoch 99/1000\n",
      "5684/5684 [==============================] - 2s 279us/step - loss: 3.4584e-05 - val_loss: 3.8694e-05\n",
      "Epoch 100/1000\n",
      "5684/5684 [==============================] - 2s 316us/step - loss: 3.3863e-05 - val_loss: 3.7095e-05\n",
      "Epoch 101/1000\n",
      "5684/5684 [==============================] - 2s 346us/step - loss: 3.3092e-05 - val_loss: 3.6505e-05\n",
      "Epoch 102/1000\n",
      "5684/5684 [==============================] - 2s 340us/step - loss: 3.3460e-05 - val_loss: 3.6788e-05\n",
      "Epoch 103/1000\n",
      "5684/5684 [==============================] - 2s 343us/step - loss: 3.2852e-05 - val_loss: 3.6612e-05\n",
      "Epoch 104/1000\n",
      "5684/5684 [==============================] - 2s 303us/step - loss: 3.3488e-05 - val_loss: 4.2846e-05\n",
      "Epoch 105/1000\n",
      "5684/5684 [==============================] - 2s 317us/step - loss: 3.4585e-05 - val_loss: 3.6506e-05\n",
      "Epoch 106/1000\n",
      "5684/5684 [==============================] - 2s 315us/step - loss: 3.2365e-05 - val_loss: 3.5778e-05\n",
      "Epoch 107/1000\n",
      "5684/5684 [==============================] - 2s 315us/step - loss: 3.4693e-05 - val_loss: 3.8569e-05\n",
      "Epoch 108/1000\n",
      "5684/5684 [==============================] - 2s 348us/step - loss: 3.2224e-05 - val_loss: 3.8592e-05\n",
      "Epoch 109/1000\n",
      "5684/5684 [==============================] - 2s 352us/step - loss: 3.2921e-05 - val_loss: 3.5836e-05\n",
      "Epoch 110/1000\n",
      "5684/5684 [==============================] - 2s 342us/step - loss: 3.2003e-05 - val_loss: 3.5002e-05\n",
      "Epoch 111/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 3.2382e-05 - val_loss: 4.1461e-05\n",
      "Epoch 112/1000\n",
      "5684/5684 [==============================] - 2s 346us/step - loss: 3.2002e-05 - val_loss: 3.3895e-05\n",
      "Epoch 113/1000\n",
      "5684/5684 [==============================] - 2s 354us/step - loss: 3.2176e-05 - val_loss: 3.4426e-05\n",
      "Epoch 114/1000\n",
      "5684/5684 [==============================] - 2s 348us/step - loss: 3.3177e-05 - val_loss: 3.8212e-05\n",
      "Epoch 115/1000\n",
      "5684/5684 [==============================] - 2s 340us/step - loss: 3.2954e-05 - val_loss: 3.4029e-05\n",
      "Epoch 116/1000\n",
      "5684/5684 [==============================] - 2s 330us/step - loss: 3.2538e-05 - val_loss: 3.6259e-05\n",
      "Epoch 117/1000\n",
      "5684/5684 [==============================] - 2s 335us/step - loss: 3.1911e-05 - val_loss: 3.9878e-05\n",
      "Epoch 118/1000\n",
      "5684/5684 [==============================] - 2s 312us/step - loss: 3.2214e-05 - val_loss: 3.5413e-05\n",
      "Epoch 119/1000\n",
      "5684/5684 [==============================] - 2s 349us/step - loss: 3.1045e-05 - val_loss: 3.6400e-05\n",
      "Epoch 120/1000\n",
      "5684/5684 [==============================] - 2s 347us/step - loss: 3.0852e-05 - val_loss: 3.4798e-05\n",
      "Epoch 121/1000\n",
      "5684/5684 [==============================] - 2s 343us/step - loss: 3.2376e-05 - val_loss: 3.8559e-05\n",
      "Epoch 122/1000\n",
      "5684/5684 [==============================] - 2s 317us/step - loss: 3.0484e-05 - val_loss: 3.4750e-05\n",
      "Epoch 123/1000\n",
      "5684/5684 [==============================] - 2s 335us/step - loss: 3.1194e-05 - val_loss: 3.2736e-05\n",
      "Epoch 124/1000\n",
      "5684/5684 [==============================] - 2s 342us/step - loss: 3.1105e-05 - val_loss: 3.4562e-05\n",
      "Epoch 125/1000\n",
      "5684/5684 [==============================] - 2s 307us/step - loss: 3.1043e-05 - val_loss: 3.3478e-05\n",
      "Epoch 126/1000\n",
      "5684/5684 [==============================] - 2s 311us/step - loss: 3.2156e-05 - val_loss: 3.4765e-05\n",
      "Epoch 127/1000\n",
      "5684/5684 [==============================] - 2s 313us/step - loss: 3.0692e-05 - val_loss: 3.7756e-05\n",
      "Epoch 128/1000\n",
      "5684/5684 [==============================] - 2s 329us/step - loss: 3.0026e-05 - val_loss: 3.8143e-05\n",
      "Epoch 129/1000\n",
      "5684/5684 [==============================] - 2s 291us/step - loss: 3.0106e-05 - val_loss: 3.2840e-05\n",
      "Epoch 130/1000\n",
      "5684/5684 [==============================] - 2s 288us/step - loss: 2.9734e-05 - val_loss: 3.3739e-05\n",
      "Epoch 131/1000\n",
      "5684/5684 [==============================] - 2s 290us/step - loss: 2.9631e-05 - val_loss: 3.2244e-05\n",
      "Epoch 132/1000\n",
      "5684/5684 [==============================] - 2s 286us/step - loss: 2.9242e-05 - val_loss: 3.2172e-05\n",
      "Epoch 133/1000\n",
      "5684/5684 [==============================] - 2s 342us/step - loss: 3.0664e-05 - val_loss: 3.5071e-05\n",
      "Epoch 134/1000\n",
      "5684/5684 [==============================] - 2s 315us/step - loss: 2.9473e-05 - val_loss: 3.1313e-05\n",
      "Epoch 135/1000\n",
      "5684/5684 [==============================] - 2s 310us/step - loss: 2.8557e-05 - val_loss: 3.1446e-05\n",
      "Epoch 136/1000\n",
      "5684/5684 [==============================] - 2s 309us/step - loss: 3.0305e-05 - val_loss: 3.4477e-05\n",
      "Epoch 137/1000\n",
      "5684/5684 [==============================] - 2s 357us/step - loss: 2.9790e-05 - val_loss: 3.2656e-05\n",
      "Epoch 138/1000\n",
      "5684/5684 [==============================] - 2s 345us/step - loss: 2.9558e-05 - val_loss: 3.1739e-05\n",
      "Epoch 139/1000\n",
      "5684/5684 [==============================] - 2s 298us/step - loss: 2.8912e-05 - val_loss: 3.6005e-05\n",
      "Epoch 140/1000\n",
      "5684/5684 [==============================] - 2s 290us/step - loss: 2.8455e-05 - val_loss: 3.4411e-05\n",
      "Epoch 141/1000\n",
      "5684/5684 [==============================] - 2s 326us/step - loss: 2.8339e-05 - val_loss: 3.0672e-05\n",
      "Epoch 142/1000\n",
      "5684/5684 [==============================] - 2s 342us/step - loss: 2.9027e-05 - val_loss: 3.2472e-05\n",
      "Epoch 143/1000\n",
      "5684/5684 [==============================] - 2s 333us/step - loss: 2.8569e-05 - val_loss: 3.0489e-05\n",
      "Epoch 144/1000\n",
      "5684/5684 [==============================] - 2s 346us/step - loss: 2.8508e-05 - val_loss: 3.4681e-05\n",
      "Epoch 145/1000\n",
      "5684/5684 [==============================] - 2s 344us/step - loss: 2.8562e-05 - val_loss: 3.0486e-05\n",
      "Epoch 146/1000\n",
      "5684/5684 [==============================] - 2s 352us/step - loss: 2.8331e-05 - val_loss: 3.4843e-05\n",
      "Epoch 147/1000\n",
      "5684/5684 [==============================] - 2s 338us/step - loss: 3.0042e-05 - val_loss: 5.0706e-05\n",
      "Epoch 148/1000\n",
      "5684/5684 [==============================] - 2s 344us/step - loss: 2.9125e-05 - val_loss: 2.9903e-05\n",
      "Epoch 149/1000\n",
      "5684/5684 [==============================] - 2s 344us/step - loss: 2.8050e-05 - val_loss: 3.0880e-05\n",
      "Epoch 150/1000\n",
      "5684/5684 [==============================] - 2s 341us/step - loss: 3.0905e-05 - val_loss: 3.1105e-05\n",
      "Epoch 151/1000\n",
      "5684/5684 [==============================] - 2s 335us/step - loss: 2.8085e-05 - val_loss: 3.3574e-05\n",
      "Epoch 152/1000\n",
      "5684/5684 [==============================] - 2s 344us/step - loss: 2.7834e-05 - val_loss: 3.0828e-05\n",
      "Epoch 153/1000\n",
      "5684/5684 [==============================] - 2s 343us/step - loss: 2.6862e-05 - val_loss: 2.9401e-05\n",
      "Epoch 154/1000\n",
      "5684/5684 [==============================] - 2s 328us/step - loss: 2.9309e-05 - val_loss: 3.3616e-05\n",
      "Epoch 155/1000\n",
      "5684/5684 [==============================] - 2s 318us/step - loss: 2.9206e-05 - val_loss: 2.9822e-05\n",
      "Epoch 156/1000\n",
      "5684/5684 [==============================] - 2s 323us/step - loss: 3.0948e-05 - val_loss: 3.0361e-05\n",
      "Epoch 157/1000\n",
      "5684/5684 [==============================] - 2s 349us/step - loss: 2.7192e-05 - val_loss: 3.0736e-05\n",
      "Epoch 158/1000\n",
      "5684/5684 [==============================] - 2s 315us/step - loss: 2.9072e-05 - val_loss: 3.5369e-05\n",
      "Epoch 159/1000\n",
      "5684/5684 [==============================] - 2s 345us/step - loss: 2.7969e-05 - val_loss: 3.0407e-05\n",
      "Epoch 160/1000\n",
      "5684/5684 [==============================] - 2s 332us/step - loss: 2.6584e-05 - val_loss: 3.1520e-05\n",
      "Epoch 161/1000\n",
      "5684/5684 [==============================] - 2s 320us/step - loss: 2.6410e-05 - val_loss: 2.8619e-05\n",
      "Epoch 162/1000\n",
      "5684/5684 [==============================] - 2s 314us/step - loss: 2.7218e-05 - val_loss: 2.9889e-05\n",
      "Epoch 163/1000\n",
      "5684/5684 [==============================] - 2s 347us/step - loss: 2.6988e-05 - val_loss: 3.1972e-05\n",
      "Epoch 164/1000\n",
      "5684/5684 [==============================] - 2s 335us/step - loss: 2.7196e-05 - val_loss: 3.0165e-05\n",
      "Epoch 165/1000\n",
      "5684/5684 [==============================] - 2s 341us/step - loss: 2.7249e-05 - val_loss: 2.8615e-05\n",
      "Epoch 166/1000\n",
      "5684/5684 [==============================] - 2s 329us/step - loss: 2.8490e-05 - val_loss: 3.0263e-05\n",
      "Epoch 167/1000\n",
      "5684/5684 [==============================] - 2s 327us/step - loss: 2.6436e-05 - val_loss: 3.1138e-05\n",
      "Epoch 168/1000\n",
      "5684/5684 [==============================] - 2s 347us/step - loss: 2.8839e-05 - val_loss: 3.4059e-05\n",
      "Epoch 169/1000\n",
      "5684/5684 [==============================] - 2s 349us/step - loss: 2.8421e-05 - val_loss: 3.2479e-05\n",
      "Epoch 170/1000\n",
      "5684/5684 [==============================] - 2s 344us/step - loss: 2.7582e-05 - val_loss: 2.9141e-05\n",
      "Epoch 171/1000\n",
      "5684/5684 [==============================] - 2s 314us/step - loss: 2.7000e-05 - val_loss: 3.1548e-05\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d03a28208>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
